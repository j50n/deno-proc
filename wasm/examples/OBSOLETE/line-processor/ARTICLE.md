# High-Performance Line Processing with WebAssembly and Deno

When processing large streams of text data, one of the most common challenges is efficiently splitting content into lines while handling chunk boundaries. Traditional approaches often struggle with memory management, performance bottlenecks, and the complexity of partial line handling. This article explores a WebAssembly-based solution that delivers exceptional performance while maintaining clean, predictable memory usage.

## The Challenge

Imagine you're processing a massive log file or streaming data from a network connection. The data arrives in chunks that don't align with line boundaries:

```
Chunk 1: "First line\nSecond li"
Chunk 2: "ne\nThird line\nPartial"  
Chunk 3: " line continues\nFinal line\n"
```

You need to:
- Split each chunk into complete lines
- Handle partial lines that span chunk boundaries
- Maintain high throughput for large datasets
- Keep memory usage predictable and bounded

Traditional JavaScript solutions often involve string concatenation, regular expressions, and dynamic memory allocationâ€”all of which can create performance bottlenecks and unpredictable memory patterns.

## The WebAssembly Solution

Our solution combines the I/O strengths of Deno with the computational efficiency of WebAssembly, written in Odin. The result is a system that processes over 25 MB/s while maintaining constant memory usage.

### Architecture Overview

The system consists of two main components:

1. **WASM Module (Odin)**: Handles the core line processing logic with fixed memory buffers
2. **Deno Interface**: Manages I/O, streaming, and provides a clean JavaScript API

```typescript
// Simple usage example
const processor = new LineProcessor(wasmInstance);
const result = processor.processChunk(dataChunk);

result.lines.forEach(line => {
  // Each line is a Uint8Array with length prefix removed
  const text = new TextDecoder().decode(line);
  console.log(`Line: "${text}"`);
});
```

## Memory Management: The Key to Performance

The most critical aspect of this solution is its approach to memory management. Instead of relying on dynamic allocation, we use a fixed memory layout that eliminates allocation overhead and provides predictable performance.

### Fixed Buffer Architecture

The WASM module allocates three fixed buffers at startup:

```odin
BUFFER_SIZE :: 64 * 1024    // 64KB input buffer
OUTPUT_SIZE :: 128 * 1024   // 128KB output buffer  
LEFTOVER_SIZE :: 4096       // 4KB leftover buffer

input_buffer: [BUFFER_SIZE]u8
output_buffer: [OUTPUT_SIZE]u8
leftover_buffer: [LEFTOVER_SIZE]u8
```

**Input Buffer (64KB)**: Receives incoming data chunks from Deno. This size balances memory usage with the ability to handle reasonably large chunks efficiently.

**Output Buffer (128KB)**: Stores processed lines in a special format where each line is prefixed with its length as a 32-bit integer. The larger size accommodates the overhead of length prefixes and ensures we can process most input chunks without buffer overflow.

**Leftover Buffer (4KB)**: Holds partial lines that span chunk boundaries. This small buffer is sufficient for most real-world scenarios where individual lines rarely exceed a few kilobytes.

### Memory Layout in Action

Here's how the memory management works during processing:

1. **Input Phase**: New chunk data is copied to the input buffer
2. **Combination Phase**: Any leftover data from the previous chunk is prepended to the new input
3. **Processing Phase**: The combined data is scanned for line endings (`\n` or `\r\n`)
4. **Output Phase**: Complete lines are written to the output buffer with length prefixes
5. **Leftover Phase**: Any remaining partial line is stored in the leftover buffer

```odin
// Simplified processing logic
for i in 0..<total_len {
    if working_data[i] == '\n' {
        // Found complete line - write length + data to output
        line_len := line_end - line_start
        
        // Write 32-bit length prefix
        length_bytes := transmute([4]u8)line_len
        output_buffer[output_pos] = length_bytes[0]
        // ... write remaining bytes
        
        // Copy line data
        mem.copy(&output_buffer[output_pos], &working_data[line_start], int(line_len))
    }
}
```

### Why Fixed Buffers Work

This approach provides several key advantages:

**Predictable Performance**: No allocation/deallocation overhead means consistent processing times regardless of data size or fragmentation.

**Memory Bounds**: Total memory usage is known at compile time (196KB + small overhead), making it suitable for memory-constrained environments.

**Cache Efficiency**: Fixed buffers improve CPU cache locality, leading to better performance on modern processors.

**No Garbage Collection**: Since we're working in WASM with fixed allocations, there are no GC pauses to disrupt processing.

## Output Format: Length-Prefixed Lines

One of the clever aspects of this solution is the output format. Instead of using delimiters (which would require escaping), each line is prefixed with its length:

```
[4 bytes: line length][line data without \n or \r]
[4 bytes: line length][line data without \n or \r]
...
```

This format provides several benefits:

- **No delimiter conflicts**: Line content can contain any characters without escaping
- **Efficient parsing**: Readers know exactly how many bytes to read for each line
- **Binary safe**: Works with any byte sequence, not just text
- **Compact**: Only 4 bytes of overhead per line

## Performance Characteristics

The solution delivers impressive performance metrics:

- **Throughput**: ~25 MB/s on typical hardware
- **Line processing**: 346,000+ lines per second
- **Memory usage**: Constant 196KB regardless of input size
- **Latency**: Sub-millisecond processing for typical chunks

These numbers come from processing real data with various line lengths and chunk boundaries, demonstrating consistent performance across different scenarios.

## Handling Edge Cases

The implementation carefully handles several edge cases that often trip up simpler solutions:

**Empty Lines**: Properly preserved and output with zero-length prefixes
**Mixed Line Endings**: Handles both Unix (`\n`) and Windows (`\r\n`) formats
**Chunk Boundaries**: Partial lines are seamlessly reconstructed across chunks
**Buffer Overflow**: Graceful handling when output would exceed buffer capacity
**Final Flush**: Empty input flushes any remaining leftover data

## Integration with Deno Streams

The WASM processor integrates seamlessly with Deno's streaming capabilities:

```typescript
async function processFileStream(filePath: string) {
  const file = await Deno.open(filePath, { read: true });
  const reader = file.readable.getReader();
  
  const processor = new LineProcessor(wasmInstance);
  
  try {
    while (true) {
      const { done, value } = await reader.read();
      if (done) break;
      
      const result = processor.processChunk(value);
      
      // Process each complete line
      for (const line of result.lines) {
        await processLine(line);
      }
    }
    
    // Flush any remaining data
    const final = processor.processChunk(new Uint8Array(0));
    for (const line of final.lines) {
      await processLine(line);
    }
  } finally {
    reader.releaseLock();
    file.close();
  }
}
```

This approach combines Deno's excellent I/O performance with WASM's computational efficiency, creating a system that excels at both reading data and processing it.

## When to Use This Approach

This solution is particularly well-suited for:

- **Large file processing**: Log analysis, data migration, ETL pipelines
- **Streaming data**: Network protocols, real-time data feeds
- **Performance-critical applications**: Where consistent low latency matters
- **Memory-constrained environments**: Embedded systems, serverless functions
- **Cross-platform deployment**: The WASM module runs anywhere Deno does

## Conclusion

By combining WebAssembly's predictable performance with Deno's streaming capabilities, we've created a line processing solution that handles the complexities of chunk boundaries while maintaining excellent performance characteristics. The fixed memory approach eliminates common performance pitfalls while providing the predictability needed for production systems.

The key insight is that many streaming processing problems can benefit from moving the computational core to WebAssembly while keeping I/O and orchestration in the host environment. This hybrid approach leverages the strengths of both platforms while avoiding their respective weaknesses.

Whether you're processing gigabytes of log files or handling real-time data streams, this pattern provides a solid foundation for high-performance text processing that scales reliably from small datasets to enterprise workloads.
